{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "from pathlib import Path\n",
    "import tensorboardX as tbx\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% data import\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "\n",
    "def import_data():\n",
    "    train_data = pd.read_csv('titanic/train.csv')\n",
    "    test_data = pd.read_csv('titanic/test.csv')\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def predict_test_data(model, test_data):\n",
    "    # model = use_gpu(model)\n",
    "    model.eval()\n",
    "    pred = np.round(model.pred(test_data.values))\n",
    "    pred = np.clip(pred, 0, 1).astype(int)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def load_data(enable_labels=None):\n",
    "    train_data, test_data = import_data()\n",
    "    if not enable_labels:\n",
    "        labels = train_data.columns.value\n",
    "        train_x = labels.delete('Survived')\n",
    "        test_x = test_data[enable_labels]\n",
    "    else:\n",
    "        train_x = train_data[enable_labels]\n",
    "        test_x = test_data[enable_labels]\n",
    "    \n",
    "    train_y = train_data['Survived']\n",
    "    # test_y = test_data['Survived']\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "def get_null_index(data):\n",
    "    null_index = data.isnull().any(axis=1)\n",
    "    return null_index\n",
    "\n",
    "\n",
    "def remove_nan_preprocess(train_x, train_y):\n",
    "    # とりあえず一つでも欠損していればそのデータは有効にしないようにする\n",
    "    # train_x = train_x.values\n",
    "    null_index = get_null_index(train_x)\n",
    "    train_x = train_x[~null_index]\n",
    "    train_y = train_y[~null_index]\n",
    "\n",
    "    MALE = .0\n",
    "    FEMALE = 1.0\n",
    "    Q = .0\n",
    "    S = 1.0\n",
    "    C = 2.0\n",
    "\n",
    "    train_x = train_x.replace('male', MALE)\n",
    "    train_x = train_x.replace('female', FEMALE)\n",
    "    train_x = train_x.replace('Q', Q)\n",
    "    train_x = train_x.replace('S', S)\n",
    "    train_x = train_x.replace('C', C)\n",
    "\n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "def preprocess_from_startup(train_x, train_y, mapping_order=False):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/startupsci/titanic-data-science-solutions\n",
    "    で記述されているようなデータの前処理を行う．\n",
    "    :param train_x:\n",
    "    :param train_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # カテゴリ変数のマッピングについては，survivedに対する相関係数順にするほうがいい気がする\n",
    "    # 均等性を考慮して，survivedの平均にマッピングするようにした\n",
    "    if mapping_order:\n",
    "        # Mrs > Miss > Master > Rare > Mr\n",
    "        gender_mapping = {'male': 0.188908, 'female': 0.742038}\n",
    "        title_mapping = {\"Mr\": 0.156673, \"Rare\": 0.347826, \"Master\": 0.575000, \n",
    "                         \"Miss\": 0.702703, \"Mrs\": 0.793651}\n",
    "        embarkation_mapping = {'S': 0.336957, 'C': 0.553571, 'Q': 0.389610}\n",
    "    else:\n",
    "        gender_mapping = {'male': 1, 'female': 0}\n",
    "        title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "        embarkation_mapping = {'S': 0, 'C': 1, 'Q': 2}\n",
    "\n",
    "    # Ticketsは，相関性が見込めないため入力データには適していない\n",
    "    # Cabinは，欠損データが多く，入力データには適していない\n",
    "    # PassengerIdは，survivedに対して相関がほぼ無いため，予測に適していない．\n",
    "    # -->これらは無効にする\n",
    "    train_x = train_x.drop(['PassengerId', 'Ticket', 'Cabin'], axis=1)\n",
    "\n",
    "    # Title(敬称)から，新たに特徴量を生成にする\n",
    "    train_x['Title'] = train_x.Name.str.extract('([A-Za-z]+)\\.', expand=False)\n",
    "    # 少数の敬称は全てRareで統一する\n",
    "    train_x['Title'] = train_x['Title'].replace(['Lady', 'Countess', 'Capt', 'Col',\n",
    "                                           'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    # 変形しているだけで同じ意味のものを変換しておく\n",
    "    train_x['Title'] = train_x['Title'].replace('Mlle', 'Miss')\n",
    "    train_x['Title'] = train_x['Title'].replace('Ms', 'Miss')\n",
    "    train_x['Title'] = train_x['Title'].replace('Mme', 'Mrs')\n",
    "    train_x['Title'] = train_x['Title'].map(title_mapping)\n",
    "    train_x['Title'] = train_x['Title'].fillna(0)\n",
    "\n",
    "    # 性別を数値データにマッピング\n",
    "    train_x['Sex'] = train_x['Sex'].map(gender_mapping)\n",
    "\n",
    "    # Nameは使用しないためdrop\n",
    "    train_x = train_x.drop(['Name'], axis=1)\n",
    "\n",
    "    # Ageの欠損値を，genderとPclass別の中央値で補完する\n",
    "    for i in gender_mapping.values():\n",
    "        for j in range(0, 3):\n",
    "            current_df = train_x[np.logical_and(train_x['Sex'] == i, train_x['Pclass'] == j + 1)]\n",
    "            guess_df = current_df['Age'].dropna()\n",
    "            med = np.round(np.median(guess_df))\n",
    "            # print(current_df['Age'].isnull().va)\n",
    "            # train_x.loc[train_x[np.logical_and(train_x['Sex'] == i, train_x['Pclass'] == j + 1)]\n",
    "            #             ['Age'].isnull(), 'Age'] = med\n",
    "            train_x.loc[(train_x.Age.isnull()) & (train_x.Sex == i) & (train_x.Pclass == j + 1), \\\n",
    "                        'Age'] = med\n",
    "\n",
    "    # 年齢層を示す特徴量Agebandを定義\n",
    "    # train_x['AgeBand'] = pd.cut(train_x['Age'], 5)\n",
    "    # 特徴量FareBandを定義する\n",
    "    # train_x['FareBand'] = pd.cut(train_x['Fare'], 4)\n",
    "\n",
    "    # 家族の人数を示す特徴量FamilySizeを定義する\n",
    "    # Parchは正の相関，SibSpは負の相関をsurvivedに対して持つため，parch + sibspはどうなんだろ\n",
    "    # FamilySizeにしてしまったことで，相関係数の絶対値が小さくなるけど...\n",
    "    train_x['FamilySize'] = train_x['Parch'] + train_x['SibSp'] + 1\n",
    "\n",
    "    # 新たな特徴量IsAlone，Age*Classを定義\n",
    "    train_x['IsAlone'] = 0\n",
    "    train_x.loc[train_x['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "    train_x['Age*Class'] = train_x.Age * train_x.Pclass\n",
    "    train_x = train_x.drop(['Age', 'Pclass'], axis=1)\n",
    "\n",
    "    # Embarkedの欠損値を，最頻値で補完する\n",
    "    train_x['Embarked'] = train_x['Embarked'].map(embarkation_mapping)\n",
    "    embarkation_mode = train_x['Embarked'].dropna().mode()[0]\n",
    "    train_x['Embarked'] =train_x['Embarked'].fillna(embarkation_mode)\n",
    "\n",
    "    # Fareの欠損値を，中央値で補完にする\n",
    "    train_x['Fare'] = train_x['Fare'].fillna(train_x['Fare'].median())\n",
    "    return train_x, train_y\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% import data and its processing\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "def training_randomforest(data_x, data_y, test_name, depth, n_forest):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(data_x, data_y,\n",
    "                                                test_size=0.2,\n",
    "                                                shuffle=True,\n",
    "                                                random_state=47,\n",
    "                                                stratify=data_y)\n",
    "\n",
    "    random_forest = RandomForestClassifier(max_depth=depth, n_estimators=n_forest, random_state=42)\n",
    "    random_forest.fit(train_x, train_y)\n",
    "    \n",
    "    trainaccuracy_random_forest = random_forest.score(train_x, train_y)\n",
    "    print('TrainAccuracy: {}'.format(trainaccuracy_random_forest))\n",
    "    \n",
    "    y_pred = random_forest.predict(test_x)\n",
    "    acc = accuracy_score(test_y, y_pred)\n",
    "    print(f'Test Accuracy: {acc}')\n",
    "    \n",
    "    eval = pd.DataFrame([[trainaccuracy_random_forest, acc]], columns=['train Accuracy', 'Test Accuracy'])\n",
    "    Path(f'random_forest/{test_name}').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    eval.to_csv(f'random_forest/{test_name}/score.csv', index=False)\n",
    "    \n",
    "    return random_forest\n",
    "\n",
    "\n",
    "def training_xgboost(data_x, data_y, test_name):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(data_x, data_y,\n",
    "                                                test_size=0.2,\n",
    "                                                shuffle=True,\n",
    "                                                random_state=47,\n",
    "                                                stratify=data_y)\n",
    "\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.fit(train_x, train_y)\n",
    "    \n",
    "    y_train_pred = model.predict(train_x)\n",
    "    y_test_pred = model.predict(test_x)\n",
    "    y_train_pred = np.clip(np.round(y_train_pred), 0, 1)\n",
    "    y_test_pred = np.clip(np.round(y_test_pred), 0, 1)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train_pred, train_y)\n",
    "    print('TrainAccuracy: {}'.format(train_acc))\n",
    "    test_acc = accuracy_score(y_test_pred, test_y)\n",
    "    print(f'Test Accuracy: {test_acc}')\n",
    "    \n",
    "    eval = pd.DataFrame([[train_acc, test_acc]], columns=['train Accuracy', 'Test Accuracy'])\n",
    "    Path(f'XGBoost/{test_name}').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    eval.to_csv(f'XGBoost/{test_name}/score.csv', index=False)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_forest(forest, test_x, passenger_id, test_name):\n",
    "    test_input = test_x.values\n",
    "    y_pred = forest.predict(test_input)\n",
    "    y_pred = np.clip(np.round(y_pred), 0, 1).astype(int)\n",
    "    \n",
    "    Path(f'random_forest/{test_name}').mkdir(parents=True, exist_ok=True)\n",
    "    pred = pd.DataFrame(y_pred, columns=['Survived'])\n",
    "    submission_pred = pd.concat([passenger_id, pred], axis=1)\n",
    "    submission_pred.to_csv(f'random_forest/{test_name}/submission.csv', index=False)\n",
    "    \n",
    "    \n",
    "def predict_xgboost(model, test_x, passenger_id, test_name):\n",
    "    test_input = test_x.values\n",
    "    y_pred = model.predict(test_input)\n",
    "    \n",
    "    y_pred = np.clip(np.round(y_pred), 0, 1).astype(int)\n",
    "    \n",
    "    Path(f'XGBoost/{test_name}').mkdir(parents=True, exist_ok=True)\n",
    "    pred = pd.DataFrame(y_pred, columns=['Survived'])\n",
    "    submission_pred = pd.concat([passenger_id, pred], axis=1)\n",
    "    submission_pred.to_csv(f'XGBoost/{test_name}/submission.csv', index=False)\n",
    "\n",
    "\n",
    "def trainging_normal_randomforest():\n",
    "    train, test = import_data()\n",
    "    train_x = train.drop(['Survived'], axis=1)\n",
    "    train_y = train['Survived']\n",
    "    train_x, train_y = preprocess_from_startup(train_x, train_y)\n",
    "    forest = training_randomforest(train_x.values, train_y.values, 'normal', 10, 10)\n",
    "    \n",
    "    test_x, _ = preprocess_from_startup(test, None)\n",
    "    predict_forest(forest, test_x, test['PassengerId'], 'normal')\n",
    "\n",
    "\n",
    "def trainging_normal_xgboost():\n",
    "    train, test = import_data()\n",
    "    train_x = train.drop(['Survived'], axis=1)\n",
    "    train_y = train['Survived']\n",
    "    train_x, train_y = preprocess_from_startup(train_x, train_y)\n",
    "    forest = training_xgboost(train_x.values, train_y.values, 'normal')\n",
    "    \n",
    "    test_x, _ = preprocess_from_startup(test, None)\n",
    "    predict_xgboost(forest, test_x, test['PassengerId'], 'normal')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% For training random forest\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6938814b59a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mLNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mDEPTH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mUNITS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "class LNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LNN, self).__init__()\n",
    "        DEPTH = 8\n",
    "        UNITS = 128\n",
    "        self.filename = 'LNN.pth'\n",
    "        self.fc_input = nn.Linear(input_dim, UNITS)\n",
    "        self.fc_array = nn.ModuleList([nn.Linear(UNITS, UNITS) for _ in range(DEPTH - 2)])\n",
    "        self.fc_output = nn.Linear(UNITS, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        y = F.relu(self.fc_input(x))\n",
    "        for layer in self.fc_array:\n",
    "            y = F.dropout(y, training=self.training)\n",
    "            y = F.relu(layer(y))\n",
    "        y = F.dropout(y, training=self.training)\n",
    "        y = self.fc_output(y)\n",
    "        return y\n",
    "\n",
    "    def pred(self, x):\n",
    "        x = torch.from_numpy(x).float()\n",
    "        pred = self.forward(x).detach().numpy()\n",
    "        pred = np.clip(np.round(pred), 0, 1)\n",
    "        return pred\n",
    "\n",
    "    def save(self, save_path):\n",
    "        Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(self.state_dict(), f'{save_path}/{self.filename}')\n",
    "\n",
    "    def load(self, load_path):\n",
    "        if os.path.isfile(f'{load_path}/{self.filename}'):\n",
    "            self.load_state_dict(torch.load(f'{load_path}/{self.filename}'))\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "class ResLNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ResLNN, self).__init__()\n",
    "        DEPTH = 8\n",
    "        UNITS = 128\n",
    "        self.filename = 'ResLNN.pth'\n",
    "        self.fc_input = nn.Linear(input_dim, UNITS)\n",
    "        self.fc_array = nn.ModuleList([nn.Linear(UNITS, UNITS) for _ in range(DEPTH - 2)])\n",
    "        self.fc_output = nn.Linear(UNITS, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        y = F.relu(self.fc_input(x))\n",
    "        res = y\n",
    "        for n, layer in enumerate(self.fc_array):\n",
    "            y = F.dropout(y, training=self.training)\n",
    "            y = F.relu(layer(y))\n",
    "            if n % 2 == 2 and n != 0:\n",
    "                y += res\n",
    "                res = y\n",
    "        y = F.dropout(y, training=self.training)\n",
    "        y = self.fc_output(y)\n",
    "        return y\n",
    "\n",
    "    def pred(self, x):\n",
    "        x = torch.from_numpy(x).float()\n",
    "        pred = self.forward(x).detach().numpy()\n",
    "        pred = np.clip(np.round(pred), 0, 1)\n",
    "        return pred\n",
    "\n",
    "    def save(self, save_path):\n",
    "        Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(self.state_dict(), f'{save_path}/{self.filename}')\n",
    "\n",
    "    def load(self, load_path):\n",
    "        if os.path.isfile(f'{load_path}/{self.filename}'):\n",
    "            self.load_state_dict(torch.load(f'{load_path}/{self.filename}'))\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "class BNLNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(BNLNN, self).__init__()\n",
    "        DEPTH = 16\n",
    "        UNITS = 256\n",
    "        self.filename = 'BnLNN.pth'\n",
    "        self.fc_input = nn.Linear(input_dim, UNITS)\n",
    "        self.fc_array = nn.ModuleList([nn.Linear(UNITS, UNITS) for _ in range(DEPTH - 2)])\n",
    "        self.bn_array = nn.ModuleList([nn.BatchNorm1d(UNITS) for _ in range(DEPTH - 2)])\n",
    "        self.fc_output = nn.Linear(UNITS, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.fc_input(x))\n",
    "        for layer, bn in zip(self.fc_array, self.bn_array):\n",
    "            y = F.relu(bn(layer(y)))\n",
    "        y = self.fc_output(y)\n",
    "        return y\n",
    "\n",
    "    def pred(self, x):\n",
    "        x = torch.from_numpy(x).float()\n",
    "        pred = self.forward(x).detach().numpy()\n",
    "        pred = np.clip(np.round(pred), 0, 1)\n",
    "        return pred\n",
    "\n",
    "    def save(self, save_path):\n",
    "        Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(self.state_dict(), f'{save_path}/{self.filename}')\n",
    "\n",
    "    def load(self, load_path):\n",
    "        if os.path.isfile(f'{load_path}/{self.filename}'):\n",
    "            self.load_state_dict(torch.load(f'{load_path}/{self.filename}'))\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "class ResBNLNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ResBNLNN, self).__init__()\n",
    "        DEPTH = 16\n",
    "        UNITS = 256\n",
    "        self.filename = 'ResBnLNN.pth'\n",
    "        self.fc_input = nn.Linear(input_dim, UNITS)\n",
    "        self.fc_array = nn.ModuleList([nn.Linear(UNITS, UNITS) for _ in range(DEPTH - 2)])\n",
    "        self.bn_array = nn.ModuleList([nn.BatchNorm1d(UNITS) for _ in range(DEPTH - 2)])\n",
    "        self.fc_output = nn.Linear(UNITS, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.fc_input(x))\n",
    "        res = y\n",
    "        n = 0\n",
    "        for layer, bn in zip(self.fc_array, self.bn_array):\n",
    "            y = F.relu(bn(layer(y)))\n",
    "\n",
    "            if n % 2 == 2 and n != 0:\n",
    "                y += res\n",
    "                res = y\n",
    "            n += 1\n",
    "\n",
    "        y = self.fc_output(y)\n",
    "        return y\n",
    "\n",
    "    def pred(self, x):\n",
    "        x = torch.from_numpy(x).float()\n",
    "        pred = self.forward(x).detach().numpy()\n",
    "        pred = np.clip(np.round(pred), 0, 1)\n",
    "        return pred\n",
    "\n",
    "    def save(self, save_path):\n",
    "        Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(self.state_dict(), f'{save_path}/{self.filename}')\n",
    "\n",
    "    def load(self, load_path):\n",
    "        if os.path.isfile(f'{load_path}/{self.filename}'):\n",
    "            self.load_state_dict(torch.load(f'{load_path}/{self.filename}'))\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def use_gpu(e):\n",
    "    if torch.cuda.is_available():\n",
    "        return e.cuda()\n",
    "    return e\n",
    "\n",
    "\n",
    "def train(model, loss_func, optimizer, trX, trY):\n",
    "    x = Variable(trX, requires_grad=False)\n",
    "    y = Variable(trY, requires_grad=False)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model.forward(x)\n",
    "    loss = loss_func(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.data\n",
    "\n",
    "\n",
    "def training(model, data_x, data_y, epochs, batch_size, model_name, eval_num=20, visualize_num=10):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(data_x, data_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=47,\n",
    "                                                    stratify=data_y)\n",
    "    train_ = data.TensorDataset(torch.from_numpy(train_x).float(),\n",
    "                                torch.from_numpy(train_y).float())\n",
    "    train_iter = torch.utils.data.DataLoader(train_, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Path(f'{save_path}/total_loss/').mkdir(exist_ok=True, parents=True)\n",
    "    # Path(f'{save_path}/metrics/').mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    if model.load(f'./models/{model_name}'):\n",
    "        print(f'loaded existing model file ./models/{model_name}')\n",
    "    else:\n",
    "        print(f'not found existing model file ./models/{model_name}')\n",
    "\n",
    "    # SummaryWriterのインスタンス作成[ポイント2]\n",
    "    writer = tbx.SummaryWriter(f'./logs/{model_name}/')\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    torch.manual_seed(1)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model = use_gpu(model)\n",
    "        model.train()\n",
    "        loss = 0\n",
    "        for i, train_data in enumerate(train_iter):\n",
    "            inputs, labels = train_data\n",
    "            inputs = use_gpu(inputs)\n",
    "            labels = use_gpu(labels)\n",
    "            loss += train(model, criterion, optimizer, inputs, labels)    \n",
    "\n",
    "        if epoch % visualize_num == 0:\n",
    "            print(f'epoch {epoch}: loss {loss / batch_size}')\n",
    "\n",
    "        if epoch % eval_num == 0:\n",
    "            model.cpu()\n",
    "            model.eval()\n",
    "            pred = model.pred(test_x)\n",
    "            # y = np.reshape(test_y, pred.shape)\n",
    "            acc = accuracy_score(test_y, pred)\n",
    "            print(f'{epoch} Accuracy Score:{acc}')\n",
    "            writer.add_scalar('Accuracy for training set', acc, int(epoch / eval_num))\n",
    "\n",
    "            model.save(f'./models/{model_name}')\n",
    "            print(f'save model at ./models/{model_name}')\n",
    "            \n",
    "            writer.add_scalar('Total Loss', loss / batch_size, epoch)\n",
    "    writer.export_scalars_to_json(f\"./logs/{model_name}/all_scalars.json\")\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def training_LNN():\n",
    "    train_x, train_y = load_data(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'])\n",
    "    train_x, train_y = remove_nan_preprocess(train_x, train_y)\n",
    "    model = LNN(7, 1)\n",
    "    training(model, train_x.values, train_y.values, 30000, 64, 'models/lnn/', eval_num=5)\n",
    "\n",
    "\n",
    "def training_BNLNN():\n",
    "    train_x, train_y = load_data(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'])\n",
    "    train_x, train_y = remove_nan_preprocess(train_x, train_y)\n",
    "    model = BNLNN(7, 1)\n",
    "    training(model, train_x.values, train_y.values, 30000, 64, 'models/bnlnn/', eval_num=50)\n",
    "\n",
    "\n",
    "def training_LNN_startup():\n",
    "    train, test = import_data()\n",
    "    train_y = train[['Survived']]\n",
    "    train_x = train.drop(['Survived'], axis=1)\n",
    "    train_x, train_y = preprocess_from_startup(train_x, train_y)\n",
    "    model = LNN(9, 1)\n",
    "    training(model, train_x.values, train_y.values, 30000, 64, 'models/lnn_startup/', eval_num=50)\n",
    "\n",
    "\n",
    "def training_BNLNN_startup():\n",
    "    train, test = import_data()\n",
    "    train_y = train[['Survived']]\n",
    "    train_x = train.drop(['Survived'], axis=1)\n",
    "    train_x, train_y = preprocess_from_startup(train_x, train_y)\n",
    "    model = BNLNN(9, 1)\n",
    "    training(model, train_x.values, train_y.values, 30000, 64, 'models/bnlnn_startup/', eval_num=50)\n",
    "\n",
    "\n",
    "def training_ResLNN_startup():\n",
    "    train, test = import_data()\n",
    "    train_y = train[['Survived']]\n",
    "    train_x = train.drop(['Survived'], axis=1)\n",
    "    train_x, train_y = preprocess_from_startup(train_x, train_y)\n",
    "    model = ResLNN(9, 1)\n",
    "    training(model, train_x.values, train_y.values, 30000, 64, 'models/Reslnn_startup/', eval_num=50)\n",
    "\n",
    "\n",
    "def training_ResBNLNN_startup():\n",
    "    train, test = import_data()\n",
    "    train_y = train[['Survived']]\n",
    "    train_x = train.drop(['Survived'], axis=1)\n",
    "    train_x, train_y = preprocess_from_startup(train_x, train_y)\n",
    "    model = ResBNLNN(9, 1)\n",
    "    training(model, train_x.values, train_y.values, 30000, 64, 'models/Resbnlnn_startup/', eval_num=200)\n",
    "\n",
    "\n",
    "def training_ResBNLNN_order():\n",
    "    train, test = import_data()\n",
    "    train_y = train[['Survived']]\n",
    "    train_x = train.drop(['Survived'], axis=1)\n",
    "    train_x, train_y = preprocess_from_startup(train_x, train_y, mapping_order=True)\n",
    "    model = ResBNLNN(9, 1)\n",
    "    training(model, train_x.values, train_y.values, 30000, 64, 'models/Resbnlnn_order/', eval_num=200)\n",
    "    \n",
    "\n",
    "def testing_ResBNLNN_startup():\n",
    "    train, test = import_data()\n",
    "    processd_test, __ = preprocess_from_startup(test, None, mapping_order=True)\n",
    "    model = ResBNLNN(9, 1)\n",
    "    model.load('models/Resbnlnn_startup/')\n",
    "    pred = predict_test_data(model, processd_test)\n",
    "    submission_pred = test['PassengerId']\n",
    "    pred = pd.DataFrame(pred, columns=['Survived'])\n",
    "    submission_pred = pd.concat([submission_pred, pred], axis=1)\n",
    "    # submission_pred['Survived'] = pred\n",
    "    submission_pred.to_csv(f'models/Resbnlnn_startup/gender_submission.csv', index=False)\n",
    "    print(submission_pred)    \n",
    "\n",
    "    \n",
    "def testing_ResBNLNN_order():\n",
    "    train, test = import_data()\n",
    "    processd_test, __ = preprocess_from_startup(test, None, mapping_order=True)\n",
    "    model = ResBNLNN(9, 1)\n",
    "    model.load('models/Resbnlnn_order/')\n",
    "    pred = predict_test_data(model, processd_test)\n",
    "    submission_pred = test['PassengerId']\n",
    "    pred = pd.DataFrame(pred, columns=['Survived'])\n",
    "    submission_pred = pd.concat([submission_pred, pred], axis=1)\n",
    "    # submission_pred['Survived'] = pred\n",
    "    submission_pred.to_csv(f'models/Resbnlnn_order/gender_submission.csv', index=False)\n",
    "    print(submission_pred)\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% For Training Neural Nets\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "Sex           0\nSibSp         0\nParch         0\nFare          0\nEmbarked      0\nTitle         0\nFamilySize    0\nIsAlone       0\nAge*Class     0\ndtype: int64"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 41
    }
   ],
   "source": [
    "train_data, test = import_data()\n",
    "train_y = train_data[['Survived']]\n",
    "train_x = train_data.drop(['Survived'], axis=1)\n",
    "train_x, train_y = preprocess_from_startup(train_x, train_y, mapping_order=True)\n",
    "\n",
    "train_x.isnull().sum()\n",
    "\n",
    "# concat_train = pd.concat([train_x, train_y], axis=1)\n",
    "# concat_train.corr()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% data analysis\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "            Pclass       Sex       Age     SibSp     Parch      Fare  \\\nPclass    1.000000 -0.150826 -0.365902  0.065187  0.023666 -0.552893   \nSex      -0.150826  1.000000 -0.099037  0.106296  0.249543  0.182457   \nAge      -0.365902 -0.099037  1.000000 -0.307351 -0.187896  0.093143   \nSibSp     0.065187  0.106296 -0.307351  1.000000  0.383338  0.139860   \nParch     0.023666  0.249543 -0.187896  0.383338  1.000000  0.206624   \nFare     -0.552893  0.182457  0.093143  0.139860  0.206624  1.000000   \nEmbarked -0.297517  0.077391  0.042340 -0.062028 -0.004120  0.286416   \nSurvived -0.356462  0.536762 -0.082446 -0.015523  0.095265  0.266100   \n\n          Embarked  Survived  \nPclass   -0.297517 -0.356462  \nSex       0.077391  0.536762  \nAge       0.042340 -0.082446  \nSibSp    -0.062028 -0.015523  \nParch    -0.004120  0.095265  \nFare      0.286416  0.266100  \nEmbarked  1.000000  0.189657  \nSurvived  0.189657  1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Pclass</th>\n      <td>1.000000</td>\n      <td>-0.150826</td>\n      <td>-0.365902</td>\n      <td>0.065187</td>\n      <td>0.023666</td>\n      <td>-0.552893</td>\n      <td>-0.297517</td>\n      <td>-0.356462</td>\n    </tr>\n    <tr>\n      <th>Sex</th>\n      <td>-0.150826</td>\n      <td>1.000000</td>\n      <td>-0.099037</td>\n      <td>0.106296</td>\n      <td>0.249543</td>\n      <td>0.182457</td>\n      <td>0.077391</td>\n      <td>0.536762</td>\n    </tr>\n    <tr>\n      <th>Age</th>\n      <td>-0.365902</td>\n      <td>-0.099037</td>\n      <td>1.000000</td>\n      <td>-0.307351</td>\n      <td>-0.187896</td>\n      <td>0.093143</td>\n      <td>0.042340</td>\n      <td>-0.082446</td>\n    </tr>\n    <tr>\n      <th>SibSp</th>\n      <td>0.065187</td>\n      <td>0.106296</td>\n      <td>-0.307351</td>\n      <td>1.000000</td>\n      <td>0.383338</td>\n      <td>0.139860</td>\n      <td>-0.062028</td>\n      <td>-0.015523</td>\n    </tr>\n    <tr>\n      <th>Parch</th>\n      <td>0.023666</td>\n      <td>0.249543</td>\n      <td>-0.187896</td>\n      <td>0.383338</td>\n      <td>1.000000</td>\n      <td>0.206624</td>\n      <td>-0.004120</td>\n      <td>0.095265</td>\n    </tr>\n    <tr>\n      <th>Fare</th>\n      <td>-0.552893</td>\n      <td>0.182457</td>\n      <td>0.093143</td>\n      <td>0.139860</td>\n      <td>0.206624</td>\n      <td>1.000000</td>\n      <td>0.286416</td>\n      <td>0.266100</td>\n    </tr>\n    <tr>\n      <th>Embarked</th>\n      <td>-0.297517</td>\n      <td>0.077391</td>\n      <td>0.042340</td>\n      <td>-0.062028</td>\n      <td>-0.004120</td>\n      <td>0.286416</td>\n      <td>1.000000</td>\n      <td>0.189657</td>\n    </tr>\n    <tr>\n      <th>Survived</th>\n      <td>-0.356462</td>\n      <td>0.536762</td>\n      <td>-0.082446</td>\n      <td>-0.015523</td>\n      <td>0.095265</td>\n      <td>0.266100</td>\n      <td>0.189657</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 42
    }
   ],
   "source": [
    "\n",
    "# print('-' * 60)\n",
    "\n",
    "train_x, train_y = load_data(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'])\n",
    "train_x, train_y = remove_nan_preprocess(train_x, train_y)\n",
    "concat_train = pd.concat([train_x, train_y], axis=1)\n",
    "concat_train.corr()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "loaded existing model file ./models/models/Resbnlnn_order/\n",
      "epoch 10: loss 0.025699926540255547\n",
      "epoch 20: loss 0.026811711490154266\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "training_ResBNLNN_order()\n",
    "# training_LNN_startup()\n",
    "# training_BNLNN_startup()\n",
    "# training_ResLNN_startup()\n",
    "# training_ResBNLNN_startup()\n",
    "# training_LNN()\n",
    "# training_BNLNN()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "      Sex  Survived\n0  female  0.742038\n1    male  0.188908",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sex</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>female</td>\n      <td>0.742038</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>male</td>\n      <td>0.188908</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 44
    }
   ],
   "source": [
    "train, test = import_data()\n",
    "\n",
    "train['Title'] = train.Name.str.extract('([A-Za-z]+)\\.', expand=False)\n",
    "train['Title'] = train['Title'].replace(['Lady', 'Countess','Capt', 'Col', \\\n",
    "'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "train['Title'] = train['Title'].replace('Mlle', 'Miss')\n",
    "train['Title'] = train['Title'].replace('Ms', 'Miss')\n",
    "train['Title'] = train['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "# train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n",
    "# train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean()\n",
    "train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean()\n",
    "\n",
    "# C > Q > S\n",
    "# Female > male\n",
    "# Mrs > Miss > Master > Rare > Mr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% mapping analysis\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "     PassengerId  Survived\n0            892         0\n1            893         0\n2            894         0\n3            895         1\n4            896         1\n..           ...       ...\n413         1305         1\n414         1306         0\n415         1307         0\n416         1308         1\n417         1309         0\n\n[418 rows x 2 columns]\n     PassengerId  Survived\n0            892         0\n1            893         0\n2            894         1\n3            895         1\n4            896         0\n..           ...       ...\n413         1305         0\n414         1306         0\n415         1307         0\n416         1308         0\n417         1309         0\n\n[418 rows x 2 columns]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "testing_ResBNLNN_startup()\n",
    "testing_ResBNLNN_order()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[12:51:38] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\nTrainAccuracy: 0.8806179775280899\nTest Accuracy: 0.8100558659217877\n",
      "TrainAccuracy: 0.9466292134831461\nTest Accuracy: 0.8044692737430168\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "trainging_normal_xgboost()\n",
    "trainging_normal_randomforest()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "pycharm-552fe350",
   "language": "python",
   "display_name": "PyCharm (titanic)"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}